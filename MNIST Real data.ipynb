{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os, glob\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.png</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.png</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.png</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.png</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  filename  label\n",
       "0    0.png      4\n",
       "1    1.png      9\n",
       "2    2.png      1\n",
       "3    3.png      7\n",
       "4    4.png      3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('C:/Users/Admin/Desktop/CWD/DATA/MNIST REAL DATA/train.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "from skimage import io\n",
    "cwd = 'C:/Users/Admin/Desktop/CWD/DATA/MNIST REAL DATA/Images/train'\n",
    "X = data['filename']\n",
    "Y = data['label']\n",
    "x = []\n",
    "for image in X:\n",
    "    x.append(os.path.join(cwd,image).replace(\"\\\\\",\"/\"))\n",
    "x = np.array(x)\n",
    "\n",
    "\n",
    "all_images = []\n",
    "for image_path in x:\n",
    "  img = io.imread(image_path , as_grey=True)\n",
    "  img = img.reshape(img.shape[0]*img.shape[1],)\n",
    "  all_images.append(img)\n",
    "x_train = np.array(all_images)\n",
    "y_train = np.array(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "(49000, 784)\n",
      "(49000, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "y_train = y_train.reshape(len(y_train), 1)\n",
    "y_train = onehot_encoder.fit_transform(y_train)\n",
    "print(y_train)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49000, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_image_size = x_train[1].shape[0]\n",
    "no_of_classes = 10\n",
    "n_h1 = 500\n",
    "n_h2 = 450\n",
    "n_h3 = 212\n",
    "n_out = no_of_classes\n",
    "x_train.shape\n",
    "# y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "x_train1, x_test1, y_train1, y_test1 = train_test_split(x_train, y_train, test_size = 0.2)\n",
    "type(x_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder('float', shape = [None, flat_image_size])\n",
    "y = tf.placeholder('float', shape = [None, no_of_classes])\n",
    "\n",
    "def neural_nets(data):\n",
    "    hidden_layer_1 = {'weights': tf.Variable(tf.random_normal([flat_image_size, n_h1])),\n",
    "                      'bias': tf.Variable(tf.random_normal([n_h1]))}\n",
    "    \n",
    "    hidden_layer_2 = {'weights': tf.Variable(tf.random_normal([n_h1, n_h2])),\n",
    "                      'bias': tf.Variable(tf.random_normal([n_h2]))}\n",
    "    \n",
    "    hidden_layer_3 = {'weights': tf.Variable(tf.random_normal([n_h2, n_h3])),\n",
    "                      'bias': tf.Variable(tf.random_normal([n_h3]))}\n",
    "    \n",
    "    output = {'weights': tf.Variable(tf.random_normal([n_h3, n_out])),\n",
    "              'bias': tf.Variable(tf.random_normal([n_out]))}\n",
    "    \n",
    "    l1 = tf.add(tf.matmul(x,hidden_layer_1['weights']), hidden_layer_1['bias'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "\n",
    "    l2 = tf.add(tf.matmul(l1,hidden_layer_2['weights']), hidden_layer_2['bias'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "    \n",
    "    l3 = tf.add(tf.matmul(l2,hidden_layer_3['weights']), hidden_layer_3['bias'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "    \n",
    "    output = tf.add(tf.matmul(l3,output['weights']), output['bias'])\n",
    "    output = tf.nn.relu(output)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-261a680233cd>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "Epoch 0 out of  282 Epochs Loss:  19126.2207031\n",
      "Epoch 1 out of  282 Epochs Loss:  15793.1328125\n",
      "Epoch 2 out of  282 Epochs Loss:  13178.2412109\n",
      "Epoch 3 out of  282 Epochs Loss:  11080.2861328\n",
      "Epoch 4 out of  282 Epochs Loss:  9324.94335938\n",
      "Epoch 5 out of  282 Epochs Loss:  7782.34619141\n",
      "Epoch 6 out of  282 Epochs Loss:  6393.62109375\n",
      "Epoch 7 out of  282 Epochs Loss:  5149.58300781\n",
      "Epoch 8 out of  282 Epochs Loss:  4055.91625977\n",
      "Epoch 9 out of  282 Epochs Loss:  3126.30419922\n",
      "Epoch 10 out of  282 Epochs Loss:  2362.43334961\n",
      "Epoch 11 out of  282 Epochs Loss:  1748.99279785\n",
      "Epoch 12 out of  282 Epochs Loss:  1269.86291504\n",
      "Epoch 13 out of  282 Epochs Loss:  901.412353516\n",
      "Epoch 14 out of  282 Epochs Loss:  627.856140137\n",
      "Epoch 15 out of  282 Epochs Loss:  430.652099609\n",
      "Epoch 16 out of  282 Epochs Loss:  293.068878174\n",
      "Epoch 17 out of  282 Epochs Loss:  199.979110718\n",
      "Epoch 18 out of  282 Epochs Loss:  137.101699829\n",
      "Epoch 19 out of  282 Epochs Loss:  95.9276885986\n",
      "Epoch 20 out of  282 Epochs Loss:  68.7451629639\n",
      "Epoch 21 out of  282 Epochs Loss:  50.3349571228\n",
      "Epoch 22 out of  282 Epochs Loss:  37.8458709717\n",
      "Epoch 23 out of  282 Epochs Loss:  29.2102432251\n",
      "Epoch 24 out of  282 Epochs Loss:  22.9829864502\n",
      "Epoch 25 out of  282 Epochs Loss:  18.2843647003\n",
      "Epoch 26 out of  282 Epochs Loss:  14.8579435349\n",
      "Epoch 27 out of  282 Epochs Loss:  12.3331890106\n",
      "Epoch 28 out of  282 Epochs Loss:  10.5302228928\n",
      "Epoch 29 out of  282 Epochs Loss:  9.14621353149\n",
      "Epoch 30 out of  282 Epochs Loss:  8.02595043182\n",
      "Epoch 31 out of  282 Epochs Loss:  7.15571022034\n",
      "Epoch 32 out of  282 Epochs Loss:  6.4815454483\n",
      "Epoch 33 out of  282 Epochs Loss:  5.92391204834\n",
      "Epoch 34 out of  282 Epochs Loss:  5.49349975586\n",
      "Epoch 35 out of  282 Epochs Loss:  5.13293933868\n",
      "Epoch 36 out of  282 Epochs Loss:  4.826567173\n",
      "Epoch 37 out of  282 Epochs Loss:  4.57445716858\n",
      "Epoch 38 out of  282 Epochs Loss:  4.35296535492\n",
      "Epoch 39 out of  282 Epochs Loss:  4.173579216\n",
      "Epoch 40 out of  282 Epochs Loss:  4.02944374084\n",
      "Epoch 41 out of  282 Epochs Loss:  3.90921282768\n",
      "Epoch 42 out of  282 Epochs Loss:  3.79984140396\n",
      "Epoch 43 out of  282 Epochs Loss:  3.70372128487\n",
      "Epoch 44 out of  282 Epochs Loss:  3.62429213524\n",
      "Epoch 45 out of  282 Epochs Loss:  3.55613684654\n",
      "Epoch 46 out of  282 Epochs Loss:  3.49244689941\n",
      "Epoch 47 out of  282 Epochs Loss:  3.43315935135\n",
      "Epoch 48 out of  282 Epochs Loss:  3.38035750389\n",
      "Epoch 49 out of  282 Epochs Loss:  3.33227944374\n",
      "Epoch 50 out of  282 Epochs Loss:  3.28792500496\n",
      "Epoch 51 out of  282 Epochs Loss:  3.24615263939\n",
      "Epoch 52 out of  282 Epochs Loss:  3.20798540115\n",
      "Epoch 53 out of  282 Epochs Loss:  3.17421245575\n",
      "Epoch 54 out of  282 Epochs Loss:  3.14517331123\n",
      "Epoch 55 out of  282 Epochs Loss:  3.12054252625\n",
      "Epoch 56 out of  282 Epochs Loss:  3.09821414948\n",
      "Epoch 57 out of  282 Epochs Loss:  3.0770111084\n",
      "Epoch 58 out of  282 Epochs Loss:  3.05861377716\n",
      "Epoch 59 out of  282 Epochs Loss:  3.04231119156\n",
      "Epoch 60 out of  282 Epochs Loss:  3.02671909332\n",
      "Epoch 61 out of  282 Epochs Loss:  3.01172947884\n",
      "Epoch 62 out of  282 Epochs Loss:  2.99832201004\n",
      "Epoch 63 out of  282 Epochs Loss:  2.98645162582\n",
      "Epoch 64 out of  282 Epochs Loss:  2.97544813156\n",
      "Epoch 65 out of  282 Epochs Loss:  2.96477818489\n",
      "Epoch 66 out of  282 Epochs Loss:  2.95443248749\n",
      "Epoch 67 out of  282 Epochs Loss:  2.944688797\n",
      "Epoch 68 out of  282 Epochs Loss:  2.9352722168\n",
      "Epoch 69 out of  282 Epochs Loss:  2.92608308792\n",
      "Epoch 70 out of  282 Epochs Loss:  2.91708850861\n",
      "Epoch 71 out of  282 Epochs Loss:  2.90833377838\n",
      "Epoch 72 out of  282 Epochs Loss:  2.89999914169\n",
      "Epoch 73 out of  282 Epochs Loss:  2.89253616333\n",
      "Epoch 74 out of  282 Epochs Loss:  2.88671994209\n",
      "Epoch 75 out of  282 Epochs Loss:  2.88094830513\n",
      "Epoch 76 out of  282 Epochs Loss:  2.87528300285\n",
      "Epoch 77 out of  282 Epochs Loss:  2.86971068382\n",
      "Epoch 78 out of  282 Epochs Loss:  2.86417222023\n",
      "Epoch 79 out of  282 Epochs Loss:  2.85876178741\n",
      "Epoch 80 out of  282 Epochs Loss:  2.85341048241\n",
      "Epoch 81 out of  282 Epochs Loss:  2.84808468819\n",
      "Epoch 82 out of  282 Epochs Loss:  2.84277105331\n",
      "Epoch 83 out of  282 Epochs Loss:  2.83746886253\n",
      "Epoch 84 out of  282 Epochs Loss:  2.83221817017\n",
      "Epoch 85 out of  282 Epochs Loss:  2.82757711411\n",
      "Epoch 86 out of  282 Epochs Loss:  2.82306671143\n",
      "Epoch 87 out of  282 Epochs Loss:  2.81856155396\n",
      "Epoch 88 out of  282 Epochs Loss:  2.81405043602\n",
      "Epoch 89 out of  282 Epochs Loss:  2.80955243111\n",
      "Epoch 90 out of  282 Epochs Loss:  2.80505418777\n",
      "Epoch 91 out of  282 Epochs Loss:  2.80055522919\n",
      "Epoch 92 out of  282 Epochs Loss:  2.7960460186\n",
      "Epoch 93 out of  282 Epochs Loss:  2.79152989388\n",
      "Epoch 94 out of  282 Epochs Loss:  2.78701043129\n",
      "Epoch 95 out of  282 Epochs Loss:  2.78253340721\n",
      "Epoch 96 out of  282 Epochs Loss:  2.77800273895\n",
      "Epoch 97 out of  282 Epochs Loss:  2.77346110344\n",
      "Epoch 98 out of  282 Epochs Loss:  2.76893210411\n",
      "Epoch 99 out of  282 Epochs Loss:  2.76441121101\n",
      "Epoch 100 out of  282 Epochs Loss:  2.75983905792\n",
      "Epoch 101 out of  282 Epochs Loss:  2.75525355339\n",
      "Epoch 102 out of  282 Epochs Loss:  2.75065422058\n",
      "Epoch 103 out of  282 Epochs Loss:  2.74603676796\n",
      "Epoch 104 out of  282 Epochs Loss:  2.74140167236\n",
      "Epoch 105 out of  282 Epochs Loss:  2.73674893379\n",
      "Epoch 106 out of  282 Epochs Loss:  2.73207783699\n",
      "Epoch 107 out of  282 Epochs Loss:  2.7273876667\n",
      "Epoch 108 out of  282 Epochs Loss:  2.72271060944\n",
      "Epoch 109 out of  282 Epochs Loss:  2.71803164482\n",
      "Epoch 110 out of  282 Epochs Loss:  2.71334385872\n",
      "Epoch 111 out of  282 Epochs Loss:  2.70863628387\n",
      "Epoch 112 out of  282 Epochs Loss:  2.70390987396\n",
      "Epoch 113 out of  282 Epochs Loss:  2.69916248322\n",
      "Epoch 114 out of  282 Epochs Loss:  2.69439721107\n",
      "Epoch 115 out of  282 Epochs Loss:  2.68961334229\n",
      "Epoch 116 out of  282 Epochs Loss:  2.68480825424\n",
      "Epoch 117 out of  282 Epochs Loss:  2.67998337746\n",
      "Epoch 118 out of  282 Epochs Loss:  2.67513918877\n",
      "Epoch 119 out of  282 Epochs Loss:  2.67027378082\n",
      "Epoch 120 out of  282 Epochs Loss:  2.66539120674\n",
      "Epoch 121 out of  282 Epochs Loss:  2.66048955917\n",
      "Epoch 122 out of  282 Epochs Loss:  2.65572476387\n",
      "Epoch 123 out of  282 Epochs Loss:  2.65111732483\n",
      "Epoch 124 out of  282 Epochs Loss:  2.64649772644\n",
      "Epoch 125 out of  282 Epochs Loss:  2.6418671608\n",
      "Epoch 126 out of  282 Epochs Loss:  2.63721895218\n",
      "Epoch 127 out of  282 Epochs Loss:  2.6325545311\n",
      "Epoch 128 out of  282 Epochs Loss:  2.62786984444\n",
      "Epoch 129 out of  282 Epochs Loss:  2.62316894531\n",
      "Epoch 130 out of  282 Epochs Loss:  2.61844825745\n",
      "Epoch 131 out of  282 Epochs Loss:  2.61371040344\n",
      "Epoch 132 out of  282 Epochs Loss:  2.60896396637\n",
      "Epoch 133 out of  282 Epochs Loss:  2.60424184799\n",
      "Epoch 134 out of  282 Epochs Loss:  2.59945225716\n",
      "Epoch 135 out of  282 Epochs Loss:  2.59464383125\n",
      "Epoch 136 out of  282 Epochs Loss:  2.58981990814\n",
      "Epoch 137 out of  282 Epochs Loss:  2.58499860764\n",
      "Epoch 138 out of  282 Epochs Loss:  2.58018922806\n",
      "Epoch 139 out of  282 Epochs Loss:  2.57532024384\n",
      "Epoch 140 out of  282 Epochs Loss:  2.57043194771\n",
      "Epoch 141 out of  282 Epochs Loss:  2.56552696228\n",
      "Epoch 142 out of  282 Epochs Loss:  2.56060504913\n",
      "Epoch 143 out of  282 Epochs Loss:  2.55566859245\n",
      "Epoch 144 out of  282 Epochs Loss:  2.55071234703\n",
      "Epoch 145 out of  282 Epochs Loss:  2.54574728012\n",
      "Epoch 146 out of  282 Epochs Loss:  2.5408513546\n",
      "Epoch 147 out of  282 Epochs Loss:  2.53586745262\n",
      "Epoch 148 out of  282 Epochs Loss:  2.53085041046\n",
      "Epoch 149 out of  282 Epochs Loss:  2.52581834793\n",
      "Epoch 150 out of  282 Epochs Loss:  2.52076840401\n",
      "Epoch 151 out of  282 Epochs Loss:  2.5157558918\n",
      "Epoch 152 out of  282 Epochs Loss:  2.51066493988\n",
      "Epoch 153 out of  282 Epochs Loss:  2.50558829308\n",
      "Epoch 154 out of  282 Epochs Loss:  2.50049877167\n",
      "Epoch 155 out of  282 Epochs Loss:  2.49536347389\n",
      "Epoch 156 out of  282 Epochs Loss:  2.49033641815\n",
      "Epoch 157 out of  282 Epochs Loss:  2.48583745956\n",
      "Epoch 158 out of  282 Epochs Loss:  2.48133349419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 159 out of  282 Epochs Loss:  2.47683572769\n",
      "Epoch 160 out of  282 Epochs Loss:  2.47236037254\n",
      "Epoch 161 out of  282 Epochs Loss:  2.46783399582\n",
      "Epoch 162 out of  282 Epochs Loss:  2.46329331398\n",
      "Epoch 163 out of  282 Epochs Loss:  2.458745718\n",
      "Epoch 164 out of  282 Epochs Loss:  2.45563220978\n",
      "Epoch 165 out of  282 Epochs Loss:  2.45280337334\n",
      "Epoch 166 out of  282 Epochs Loss:  2.44996404648\n",
      "Epoch 167 out of  282 Epochs Loss:  2.44711494446\n",
      "Epoch 168 out of  282 Epochs Loss:  2.44426727295\n",
      "Epoch 169 out of  282 Epochs Loss:  2.44143819809\n",
      "Epoch 170 out of  282 Epochs Loss:  2.43858528137\n",
      "Epoch 171 out of  282 Epochs Loss:  2.4356918335\n",
      "Epoch 172 out of  282 Epochs Loss:  2.43279027939\n",
      "Epoch 173 out of  282 Epochs Loss:  2.42990088463\n",
      "Epoch 174 out of  282 Epochs Loss:  2.42700457573\n",
      "Epoch 175 out of  282 Epochs Loss:  2.42410087585\n",
      "Epoch 176 out of  282 Epochs Loss:  2.42118430138\n",
      "Epoch 177 out of  282 Epochs Loss:  2.41825914383\n",
      "Epoch 178 out of  282 Epochs Loss:  2.41533088684\n",
      "Epoch 179 out of  282 Epochs Loss:  2.41241359711\n",
      "Epoch 180 out of  282 Epochs Loss:  2.40948867798\n",
      "Epoch 181 out of  282 Epochs Loss:  2.40651583672\n",
      "Epoch 182 out of  282 Epochs Loss:  2.40349984169\n",
      "Epoch 183 out of  282 Epochs Loss:  2.40046858788\n",
      "Epoch 184 out of  282 Epochs Loss:  2.39768195152\n",
      "Epoch 185 out of  282 Epochs Loss:  2.39503479004\n",
      "Epoch 186 out of  282 Epochs Loss:  2.39237809181\n",
      "Epoch 187 out of  282 Epochs Loss:  2.38971352577\n",
      "Epoch 188 out of  282 Epochs Loss:  2.38748955727\n",
      "Epoch 189 out of  282 Epochs Loss:  2.38537502289\n",
      "Epoch 190 out of  282 Epochs Loss:  2.3832461834\n",
      "Epoch 191 out of  282 Epochs Loss:  2.381108284\n",
      "Epoch 192 out of  282 Epochs Loss:  2.37896108627\n",
      "Epoch 193 out of  282 Epochs Loss:  2.3768055439\n",
      "Epoch 194 out of  282 Epochs Loss:  2.37464475632\n",
      "Epoch 195 out of  282 Epochs Loss:  2.37247419357\n",
      "Epoch 196 out of  282 Epochs Loss:  2.37028694153\n",
      "Epoch 197 out of  282 Epochs Loss:  2.368090868\n",
      "Epoch 198 out of  282 Epochs Loss:  2.36589097977\n",
      "Epoch 199 out of  282 Epochs Loss:  2.36368441582\n",
      "Epoch 200 out of  282 Epochs Loss:  2.36146879196\n",
      "Epoch 201 out of  282 Epochs Loss:  2.35924506187\n",
      "Epoch 202 out of  282 Epochs Loss:  2.35701203346\n",
      "Epoch 203 out of  282 Epochs Loss:  2.3547680378\n",
      "Epoch 204 out of  282 Epochs Loss:  2.35251641273\n",
      "Epoch 205 out of  282 Epochs Loss:  2.35025763512\n",
      "Epoch 206 out of  282 Epochs Loss:  2.3479924202\n",
      "Epoch 207 out of  282 Epochs Loss:  2.34572172165\n",
      "Epoch 208 out of  282 Epochs Loss:  2.34344339371\n",
      "Epoch 209 out of  282 Epochs Loss:  2.34116029739\n",
      "Epoch 210 out of  282 Epochs Loss:  2.33886671066\n",
      "Epoch 211 out of  282 Epochs Loss:  2.33656644821\n",
      "Epoch 212 out of  282 Epochs Loss:  2.33426189423\n",
      "Epoch 213 out of  282 Epochs Loss:  2.33195066452\n",
      "Epoch 214 out of  282 Epochs Loss:  2.32963132858\n",
      "Epoch 215 out of  282 Epochs Loss:  2.32730460167\n",
      "Epoch 216 out of  282 Epochs Loss:  2.32497191429\n",
      "Epoch 217 out of  282 Epochs Loss:  2.32263278961\n",
      "Epoch 218 out of  282 Epochs Loss:  2.32028770447\n",
      "Epoch 219 out of  282 Epochs Loss:  2.31794571877\n",
      "Epoch 220 out of  282 Epochs Loss:  2.31560087204\n",
      "Epoch 221 out of  282 Epochs Loss:  2.31323933601\n",
      "Epoch 222 out of  282 Epochs Loss:  2.31175684929\n",
      "Epoch 223 out of  282 Epochs Loss:  2.31030058861\n",
      "Epoch 224 out of  282 Epochs Loss:  2.30884099007\n",
      "Epoch 225 out of  282 Epochs Loss:  2.30737733841\n",
      "Epoch 226 out of  282 Epochs Loss:  2.3059091568\n",
      "Epoch 227 out of  282 Epochs Loss:  2.30443668365\n",
      "Epoch 228 out of  282 Epochs Loss:  2.30296087265\n",
      "Epoch 229 out of  282 Epochs Loss:  2.30148506165\n",
      "Epoch 230 out of  282 Epochs Loss:  2.30000305176\n",
      "Epoch 231 out of  282 Epochs Loss:  2.29851531982\n",
      "Epoch 232 out of  282 Epochs Loss:  2.29702448845\n",
      "Epoch 233 out of  282 Epochs Loss:  2.29553103447\n",
      "Epoch 234 out of  282 Epochs Loss:  2.29403400421\n",
      "Epoch 235 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 236 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 237 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 238 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 239 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 240 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 241 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 242 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 243 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 244 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 245 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 246 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 247 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 248 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 249 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 250 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 251 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 252 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 253 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 254 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 255 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 256 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 257 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 258 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 259 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 260 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 261 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 262 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 263 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 264 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 265 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 266 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 267 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 268 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 269 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 270 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 271 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 272 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 273 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 274 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 275 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 276 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 277 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 278 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 279 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 280 out of  282 Epochs Loss:  2.29371261597\n",
      "Epoch 281 out of  282 Epochs Loss:  2.29371261597\n",
      "Accuracy on train-set: 10.2%\n",
      "Accuracy on test-set: 10.5%\n"
     ]
    }
   ],
   "source": [
    "def neural_nets_train(data):\n",
    "    prediction = neural_nets(data)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = prediction, labels = y))\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    no_epoch = 282\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch in range(no_epoch):\n",
    "            epoch_loss = 0\n",
    "            o,c = sess.run([optimizer, cost], feed_dict = {x:x_train1, y:y_train1})\n",
    "            epoch_loss +=c\n",
    "            print('Epoch',epoch,'out of ',no_epoch,'Epochs','Loss: ',epoch_loss)\n",
    "            \n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "       \n",
    "        feed_dict_test= {x: x_test1,\n",
    "                         y: y_test1}\n",
    "        \n",
    "        feed_dict_train = {x: x_train1,\n",
    "                     y: y_train1}\n",
    "        acc = sess.run(accuracy, feed_dict = feed_dict_train)\n",
    "        print(\"Accuracy on train-set: {0:.1%}\".format(acc))\n",
    "        \n",
    "        acc = sess.run(accuracy, feed_dict = feed_dict_test)\n",
    "        print(\"Accuracy on test-set: {0:.1%}\".format(acc))\n",
    "neural_nets_train(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
